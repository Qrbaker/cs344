{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Spam Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "{'AND': 0.2, 'SPAMIAM': 0.6666666666666666, 'NOT': 0.6666666666666666, 'SPAM': 0.6666666666666666, 'I': 0.5, 'GREEN': 0.2, 'THAT': 0.6666666666666666, 'HAM': 0.2, 'LIKE': 0.3333333333333333, 'DO': 0.3333333333333333, 'EGGS': 0.2, 'AM': 0.6666666666666666}\n",
      "\n",
      "======TESTS======\n",
      "\n",
      "\"i\"\n",
      "Spam Score: 0.5\n",
      "============\n",
      "\"I am sam, sam I am, do you like green eggs and ham?\"\n",
      "Spam Score: 0.0038910505836575863\n",
      "============\n",
      "\"spamiam\"\n",
      "Spam Score: 0.6666666666666666\n",
      "============\n",
      "\"I am spam, spam I am\"\n",
      "Spam Score: 0.9411764705882353\n",
      "============\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "spam_corpus = [[\"I\", \"am\", \"spam\", \"spam\", \"I\", \"am\"], [\"I\", \"do\", \"not\", \"like\", \"that\", \"spamiam\"]]\n",
    "ham_corpus = [[\"do\", \"i\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"], [\"i\", \"do\"]]\n",
    "\n",
    "THRESHOLD = 1\n",
    "BASE_PROB = 0.5\n",
    "\n",
    "\n",
    "class BayesSpam:\n",
    "\n",
    "    def __init__(self, ham, spam):\n",
    "\n",
    "        self.threshold = THRESHOLD\n",
    "        self.unknown_probability_value = BASE_PROB\n",
    "        self.ngood = len(ham)\n",
    "        self.nbad = len(spam)\n",
    "\n",
    "        self.ham_hash = self.hash_occurances(ham)\n",
    "        self.spam_hash = self.hash_occurances(spam)\n",
    "\n",
    "        # get a list of all words, from either corpus\n",
    "        # Credit to https://stackoverflow.com/a/16902603\n",
    "        self.token_list = set().union(*[self.ham_hash, self.spam_hash])\n",
    "\n",
    "        # Create the combined score hashmap\n",
    "        self.score_hash = {}\n",
    "\n",
    "        for word in self.token_list:\n",
    "            # Ternary operation to assign good value\n",
    "            g = (2 * self.ham_hash[word] if word in self.ham_hash else self.unknown_probability_value)\n",
    "            # Ternary operation to assign bad value\n",
    "            b = (self.spam_hash[word] if word in self.spam_hash else self.unknown_probability_value)\n",
    "\n",
    "            if g + b > self.threshold:\n",
    "                self.score_hash[word] = max(0.01, min(0.99, min(1.0, b / self.nbad) /\n",
    "                                                      (min(1.0, g / self.ngood) + min(1.0, b / self.nbad))))\n",
    "\n",
    "        print(\"Scores:\")\n",
    "        print(self.score_hash)\n",
    "\n",
    "    @staticmethod\n",
    "    def hash_occurances(corpus):\n",
    "        new_hash = {}\n",
    "        for i in corpus:\n",
    "            for v in i:\n",
    "                if v in new_hash:\n",
    "                    new_hash[v.upper()] += 1\n",
    "                else:\n",
    "                    new_hash[v.upper()] = 1\n",
    "        return new_hash\n",
    "\n",
    "    def filter_spam(self, mail):\n",
    "        mail = mail.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
    "        words = mail.upper().split()\n",
    "        prod_list = 1\n",
    "        compliment_list = 1\n",
    "\n",
    "        for word in words:\n",
    "            if word in self.score_hash:\n",
    "                prob = self.score_hash[word]\n",
    "            else:\n",
    "                prob = self.unknown_probability_value\n",
    "\n",
    "            prod_list *= prob\n",
    "            compliment_list *= (1 - prob)\n",
    "\n",
    "        return prod_list / (prod_list + compliment_list)\n",
    "    \n",
    "    def to_str(self, text):\n",
    "        return '\"' + text + \"\\\"\\nSpam Score: \" + str(self.filter_spam(text)) + \"\\n============\"\n",
    "    \n",
    "spam_filter = BayesSpam(ham_corpus, spam_corpus)\n",
    "print(\"\\n======TESTS======\\n\")\n",
    "print(spam_filter.to_str(\"i\"))\n",
    "print(spam_filter.to_str(\"I am sam, sam I am, do you like green eggs and ham?\"))\n",
    "print(spam_filter.to_str(\"spamiam\"))\n",
    "print(spam_filter.to_str(\"I am spam, spam I am\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach to spam filtering is Bayesian because it is based on **statistics** rather than hard properties. As this [deep AI article states][ai-link], \"probability is a subjective process that can change as new information is gathered, rather than a fixed value based upon frequency or propensity.\" \n",
    "\n",
    "The big benefit of this is that the model will adapt to what it is told is spam, and therefore can change with the nature of spam for a given user. Graham notes in the [\"plan for spam\" FAQ][faq-link] that spammers can't really defeat this *even if they know about it* and try to tune their emails to get through -- the filters are A) different for each user and B) would simply start filtering according to the new spam paradigms.\n",
    "\n",
    "[ai-link]:https://deepai.org/machine-learning-glossary-and-terms/bayesian-statistics\n",
    "[faq-link]:http://www.paulgraham.com/spamfaq.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bayesian Networks\n",
    "\n",
    "## a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.i:\t False: 0.5, True: 0.5\n",
      "2.ii:\t False: 0.9, True: 0.1\n",
      "2.iii:\t False: 0.952, True: 0.0476\n",
      "2.iv:\t False: 0.01, True: 0.99\n",
      "2.v:\t False: 0.639, True: 0.361\n"
     ]
    }
   ],
   "source": [
    "from probability import BayesNet, enumeration_ask, elimination_ask, gibbs_ask\n",
    "\n",
    "# Utility variables\n",
    "T, F = True, False\n",
    "\n",
    "wet_grass = BayesNet([\n",
    "    ('Cloudy', '', 0.5),\n",
    "    ('Sprinkler', 'Cloudy', {T: 0.1, F: 0.5}),\n",
    "    ('Rain', 'Cloudy', {T: 0.8, F: 0.2}),\n",
    "    ('Wet_Grass', 'Sprinkler Rain', {(T, T): 0.99, (T, F): 0.9, (F, T): 0.9, (F, F): 0.0})\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b.\n",
    "\n",
    "\\# of independent values for the full joint distribution = 2 * 2 * 2 * 2 = 2\\*\\*4 = **16**\n",
    "\n",
    "* This number comes from the 4 variables, which have 2 states each.\n",
    "\n",
    "## c.\n",
    "\n",
    "\\# of independent values in the Bayesian network = (Cloudy) + (Sprinkler) + (Rain) + (Wet Grass) = 1 + 2 + 2 + 4 = **9**\n",
    "\n",
    "* This number is derived from the conditional probability tables in 14.12 (a)\n",
    "\n",
    "## d.\n",
    "\n",
    "First, the computed results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.d.i:\t\t False: 0.5, True: 0.5\n",
      "2.d.ii:\t\t False: 0.9, True: 0.1\n",
      "2.d.iii:\t False: 0.952, True: 0.0476\n",
      "2.d.iv:\t\t False: 0.01, True: 0.99\n",
      "2.d.v:\t\t False: 0.639, True: 0.361\n"
     ]
    }
   ],
   "source": [
    "print(\"2.d.i:\\t\\t\", enumeration_ask('Cloudy', dict(), wet_grass).show_approx())\n",
    "print(\"2.d.ii:\\t\\t\", enumeration_ask('Sprinkler', dict(Cloudy=T), wet_grass).show_approx())\n",
    "print(\"2.d.iii:\\t\",enumeration_ask('Cloudy', dict(Sprinkler=T, Rain=F), wet_grass).show_approx())\n",
    "print(\"2.d.iv:\\t\\t\",enumeration_ask('Wet_Grass', dict(Cloudy=T, Sprinkler=T, Rain=T), wet_grass).show_approx())\n",
    "print(\"2.d.v:\\t\\t\",enumeration_ask('Cloudy', dict(Wet_Grass=F), wet_grass).show_approx())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now verified manually:\n",
    "\n",
    "### i.\n",
    "`P(Cloudy) = <0.5, 0.5>` \n",
    "\n",
    "Straightforward, from the table\n",
    "\n",
    "### ii.\n",
    "`P(Sprinkler | cloudy)` = `α<P(S|c), P(¬S|c)>` = `<0.1, 0.9>` \n",
    "\n",
    "Also fairly simple, just P(S) given C == t, then 1 minus that.\n",
    "\n",
    "### iii.\n",
    "`P(Cloudy | sprinkler ^ rain)   \n",
    "        = α<P(C|s,¬r), P(¬C|s,¬r)> \n",
    "        = α<(0.5)(0.1)(0.2),(0.5)(0.5)(0.8)> \n",
    "        = α<0.01, 0.2> \n",
    "        = <0.0476, 0.952>`\n",
    "\n",
    "Less simple now...\n",
    "\n",
    "### iv.\n",
    "`P(Wet Grass | cloudy ^ sprinkler ^ rain) \n",
    "        = αP(c)P(s)P(r)<P(W|s,r),P(¬W|s,r)\n",
    "        = α(0.5)(0.1)(0.8)<0.99,0.1>\n",
    "        = α<0.99, 0.1>\n",
    "        = <0.99, 0.10>`\n",
    "        \n",
    "In the end, the alpha values and the P(x)s essentially cancelled out.\n",
    "\n",
    "### v.\n",
    "`P(Cloud | ¬Wet Grass)\n",
    "        = α Σs(Σr( P(C) * P(s^r) * P(w | s^r) ) )`\n",
    "\n",
    "#### Table of Probability Summations\n",
    "\n",
    "|P(C) * P(S^R) | S=T | S=F |\n",
    "| ------ | --- | --- |\n",
    "| R = T  | (0.5)(0.08)(0.01) | (0.5)(0.02)(0.1) |\n",
    "| R = F  | (0.5)(0.72)(0.1)  | (0.5)(0.18)(1.)  |\n",
    "\n",
    "\n",
    "| P(¬C) * P(S^R) | S=T | S=F |\n",
    "| ------ | --- | --- |\n",
    "| R = T  | (0.5)(0.1)(0.01) | (0.5)(0.4)(0.1) |\n",
    "| R = F  | (0.5)(0.1)(0.1)  | (0.5)(0.4)(1.)  |\n",
    "\n",
    "\n",
    "`.       \n",
    "        = α <0.127, 0.226>\n",
    "        = <0.36, 0.64>`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
