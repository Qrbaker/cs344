{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Homework 4\n",
        "Quentin Baker\n",
        "\n",
        "## Problem 1\n",
        "Deep networks are already clearly on a different trajectory compared to perceptrons and expert systems. This is due to several important factors: modeling, data science, and connectivity. Our models are better -- perceptrons have given way to multi-level neural nets with non-linear activations, and can model successfully problems that stumped perceptrons, most famously the XOR. Data science has improved; we store more data and can access it faster, meaning its possible to train with larger sets. Third, and in my opinion most important, computers are now embedded in all sorts of things and producing meaningful data. We have access to sources of data that would have been unthinkable even a decade ago, and this is just the opening salvo. Regardless of opinion on this data collection, it is undisputable that it means a lot for NNs.\n",
        "\n",
        "## Problem 2\n",
        "![Page 1](Part2Page1.jpg)\n",
        "\n",
        "![Page 2](Part2Page2.jpg)\n",
        "\n",
        "## Problem 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      },
      "source": [
        "Create a sequential NN for our classifier (from kvlinden-courses/cs344-code/unit09)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        },
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\users\\quentin\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                650       \n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "model \u003d models.Sequential()\n",
        "\n",
        "# Configure a convnet with 3 layers of convolutions and max pooling.\n",
        "model.add(layers.Conv2D(32, (3, 3), activation\u003d\u0027relu\u0027, input_shape\u003d(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation\u003d\u0027relu\u0027))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation\u003d\u0027relu\u0027))\n",
        "\n",
        "# Add layers to flatten the 2D image and then do a 10-way classification.\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation\u003d\u0027relu\u0027))\n",
        "model.add(layers.Dense(10, activation\u003d\u0027softmax\u0027))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "\n",
        "Import the dataset and build a basic Keras structure (from https://keras.io/datasets/#fashion-mnist-database-of-fashion-articles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) \u003d fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      },
      "source": [
        "According to the [MNIST repo][mnist-git], the data is split between 60k training and 10k test examples. We can confirm this ourselves:\n",
        "\n",
        "[mnist-git]:https://github.com/zalandoresearch/fashion-mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training set:\t60000 images\n",
            "test set:\t10000 images\n"
          ]
        }
      ],
      "source": [
        "print(\u0027training set:\\t%s images\u0027 % len(train_images))\n",
        "print(\u0027test set:\\t%s images\u0027 % len(test_images))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now before continuing, I\u0027m going to reshape the data. As this is similar to the classic MNIST digits set (28x28 pixel grayscale images) I will reshape in a similar manner, then convert the set to a categorical model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "train_images \u003d train_images.reshape((60000, 28, 28, 1))\n",
        "train_images \u003d train_images.astype(\u0027float32\u0027) / 255\n",
        "\n",
        "test_images \u003d test_images.reshape((10000, 28, 28, 1))\n",
        "test_images \u003d test_images.astype(\u0027float32\u0027) / 255\n",
        "\n",
        "train_labels \u003d to_categorical(train_labels)\n",
        "test_labels \u003d to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now for the secret sauce: Hyperparams and training! I chose binary crossentropy for the loss model as I found in testing it seemed to perform better than categorical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 36s 596us/step - loss: 0.0389 - acc: 0.9846\n",
            "Epoch 2/5\n",
            "60000/60000 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 42s 705us/step - loss: 0.0361 - acc: 0.9858\n",
            "Epoch 3/5\n",
            "60000/60000 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 42s 706us/step - loss: 0.0339 - acc: 0.9869\n",
            "Epoch 4/5\n",
            "60000/60000 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 42s 708us/step - loss: 0.0317 - acc: 0.9878\n",
            "Epoch 5/5\n",
            "60000/60000 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 46s 761us/step - loss: 0.0295 - acc: 0.9887\n",
            "10000/10000 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 2s 220us/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.052288673339784146, 0.9822200072288513]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.compile(optimizer\u003d\u0027rmsprop\u0027,\n",
        "              loss\u003d\u0027binary_crossentropy\u0027,\n",
        "              metrics\u003d[\u0027accuracy\u0027])\n",
        "model.fit(train_images, train_labels, epochs\u003d5, batch_size\u003d50)\n",
        "model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "I\u0027m not sure if I had a fluke, but I ran this several times and saw \u003e\u003d 98% accuracy each time! I actually *lowered* the batch size since it is performing so well."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}