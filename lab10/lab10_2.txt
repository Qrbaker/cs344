a. AdaGrad dynamically adjusts the learning rate for each parameter.

b. Task 1:
    learning_rate=0.007,
    steps=3000,
    batch_size=40,
    hidden_units=[10, 10]

   Task 2 - AdaGrad:
    learning_rate=0.45,
    steps=300,
    batch_size=60,
    hidden_units=[10, 10]

   Task 2 - Adam:
    learning_rate=0.01,
    steps=500,
    batch_size=100,
    hidden_units=[10, 10]

   Task 3:
    processed_features["households"] = log_normalize(examples_dataframe["households"])
    processed_features["median_income"] = log_normalize(examples_dataframe["median_income"])
    processed_features["total_bedrooms"] = log_normalize(examples_dataframe["total_bedrooms"])
    processed_features["latitude"] = linear_scale(examples_dataframe["latitude"])
    processed_features["longitude"] = linear_scale(examples_dataframe["longitude"])
    processed_features["housing_median_age"] = linear_scale(examples_dataframe["housing_median_age"])
    processed_features["population"] = linear_scale(clip(examples_dataframe["population"], 0, 5000))
    processed_features["rooms_per_person"] = linear_scale(clip(examples_dataframe["rooms_per_person"], 0, 4))
    processed_features["total_rooms"] = linear_scale(clip(examples_dataframe["total_rooms"], 0, 8000))
